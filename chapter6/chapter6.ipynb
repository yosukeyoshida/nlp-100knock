{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第6章: 英語テキストの処理\n",
    "英語のテキスト（nlp.txt）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "From Wikipedia, the free encyclopedia\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. \n",
      "As such, NLP is related to the area of humani-computer interaction. \n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
      "History\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. \n",
      "In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. \n",
      "The authors claimed that within three or five years, machine translation would be a solved problem. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def nlp_reader():\n",
    "    with open('nlp.txt') as f:\n",
    "        buf = ''\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            while True:\n",
    "                m = re.search(r\"(?:\\.|;|:|\\?|!)\\s+(?=[A-Z])\", line)\n",
    "                if m is None:\n",
    "                    break\n",
    "                else:\n",
    "                    buf += line[:m.end(0)]\n",
    "                    yield buf\n",
    "                    buf = ''\n",
    "                    line = line[m.end(0):]\n",
    "            if len(line) > 0:\n",
    "                yield line\n",
    "    raise StopIteration\n",
    "\n",
    "# 最初の10行のみ\n",
    "for i, line in enumerate(nlp_reader()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "\n",
      "\n",
      "From\n",
      "Wikipedia\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "\n",
      "\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "(NLP)\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "def word_generator():\n",
    "    for i, line in enumerate(nlp_reader()):\n",
    "        words = line.rstrip().split(' ')\n",
    "        for word in words:\n",
    "            yield word.rstrip('.,;:?!')\n",
    "        yield '\\n'\n",
    "    raise StopIteration\n",
    "\n",
    "# 最初の15個のみ\n",
    "for i, word in enumerate(word_generator()):\n",
    "    if i >= 15:\n",
    "        break\n",
    "    print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tNatur\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\n",
      "\n",
      "From\tFrom\n",
      "Wikipedia\tWikipedia\n",
      "the\tthe\n",
      "free\tfree\n",
      "encyclopedia\tencyclopedia\n",
      "\n",
      "\n",
      "Natural\tNatur\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "\n",
    "def stem_generator():\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    for word in word_generator():\n",
    "        if word == '\\n':\n",
    "            yield word\n",
    "        else:\n",
    "            stem_word = stemmer.stem(word)\n",
    "            yield '{}\\t{}'.format(word, stem_word)\n",
    "    raise StopIteration\n",
    "\n",
    "# 最初の10個を表示\n",
    "for i, line in enumerate(stem_generator()):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. Tokenization\n",
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "* 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "* 主語: 述語からnsubj関係にある子（dependent）\n",
    "* 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
