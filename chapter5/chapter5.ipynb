{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第5章: 係り受け解析\n",
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import CaboCha\n",
    "\n",
    "def parse():\n",
    "    parser = CaboCha.Parser()\n",
    "    with open('neko.txt') as input, open('neko.txt.cabocha', 'w') as f:\n",
    "        for line in input:\n",
    "            f.write(parser.parse(line).toString(CaboCha.FORMAT_LATTICE))\n",
    "\n",
    "parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'surface: {} base: {}, pos: {}, pos1: {}'.format(self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "    def is_symbol(self):\n",
    "        return self.pos == '記号'\n",
    "    \n",
    "    def is_noun(self):\n",
    "        return self.pos == '名詞'\n",
    "    \n",
    "    def is_verb(self):\n",
    "        return self.pos == '動詞'\n",
    "\n",
    "    def is_joshi(self):\n",
    "        return self.pos == '助詞'\n",
    "    \n",
    "    def is_sahen_noun(self):\n",
    "        return self.is_noun() and self.pos1 == 'サ変接続'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: 　 base: 　, pos: 記号, pos1: 空白\n",
      "surface: 吾輩 base: 吾輩, pos: 名詞, pos1: 代名詞\n",
      "surface: は base: は, pos: 助詞, pos1: 係助詞\n",
      "surface: 猫 base: 猫, pos: 名詞, pos1: 一般\n",
      "surface: で base: だ, pos: 助動詞, pos1: *\n",
      "surface: ある base: ある, pos: 助動詞, pos1: *\n",
      "surface: 。 base: 。, pos: 記号, pos1: 句点\n"
     ]
    }
   ],
   "source": [
    "def cabocha_morph_reader():\n",
    "    morphs = []\n",
    "    with open('neko.txt.cabocha') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line == 'EOS':\n",
    "                yield morphs\n",
    "                morphs = []\n",
    "                continue\n",
    "            if line.startswith('*'):\n",
    "                continue\n",
    "            surface, others = line.split('\\t')\n",
    "            others = others.split(',')\n",
    "            morphs.append(Morph(surface,  others[6], others[0], others[1]))\n",
    "        raise StopIteration\n",
    "\n",
    "for i, morphs in enumerate(cabocha_morph_reader(), 1):\n",
    "    if i == 3:\n",
    "        for morph in morphs:\n",
    "            print(morph)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self):\n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'morphs: {} dst: {} srcs: {}'.format(self.surface(), self.dst, self.srcs)\n",
    "    \n",
    "    def surface(self):\n",
    "        ret = ''\n",
    "        for morph in self.morphs:\n",
    "            if morph.is_symbol():\n",
    "                continue\n",
    "            ret += morph.surface\n",
    "        return ret\n",
    "    \n",
    "    def is_include_noun(self):\n",
    "        return any([morph.is_noun() for morph in self.morphs])\n",
    "    \n",
    "    def is_include_verb(self):\n",
    "        return any([morph.is_verb() for morph in self.morphs])\n",
    "\n",
    "    def is_include_joshi(self):\n",
    "        return any([morph.is_joshi() for morph in self.morphs])\n",
    "    \n",
    "    def is_include_sahen_wo(self):\n",
    "        return len(self.sahen_wo()) > 0\n",
    "\n",
    "    def predicate(self):\n",
    "        # 最初の動詞\n",
    "        for morph in self.morphs:\n",
    "            if morph.is_verb():\n",
    "                return morph.base\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    def joshi(self):\n",
    "        # 最後の助詞\n",
    "        for morph in self.morphs[::-1]:\n",
    "            if morph.is_joshi():\n",
    "                return morph.surface\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    def sahen_wo(self):\n",
    "        morphs_size = len(self.morphs)\n",
    "        for i, morph in enumerate(self.morphs):\n",
    "            if i < morphs_size-1 and morph.is_sahen_noun() and self.morphs[i+1].surface == 'を':\n",
    "                return morph.surface + self.morphs[i+1].surface\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    def masked_noun_phrase(self, val='X'):\n",
    "        ret = []\n",
    "        for morph in self.morphs:\n",
    "            if morph.is_noun():\n",
    "                ret.append(val)\n",
    "            else:\n",
    "                ret.append(morph.surface)\n",
    "        return ''.join(ret)\n",
    "    \n",
    "    def masked_noun(self, val='X'):\n",
    "        for morph in self.morphs:\n",
    "            if morph.is_noun():\n",
    "                return val\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 morphs: 吾輩は dst: 5 srcs: []\n",
      "1 morphs: ここで dst: 2 srcs: []\n",
      "2 morphs: 始めて dst: 3 srcs: [1]\n",
      "3 morphs: 人間という dst: 4 srcs: [2]\n",
      "4 morphs: ものを dst: 5 srcs: [3]\n",
      "5 morphs: 見た dst: -1 srcs: [0, 4]\n"
     ]
    }
   ],
   "source": [
    "class ChunkStore:\n",
    "    def __init__(self):\n",
    "        self.chunks = {}\n",
    "\n",
    "    def update_chunk(self, id, **args):\n",
    "        id = int(id)\n",
    "        if id in self.chunks:\n",
    "            pass\n",
    "        else:\n",
    "            self.chunks[id] = Chunk()\n",
    "\n",
    "        if 'dst' in args:\n",
    "            self.chunks[id].dst = int(args['dst'])\n",
    "        if 'morph' in args:\n",
    "            self.chunks[id].morphs.append(args['morph'])\n",
    "        if 'src' in args:\n",
    "            self.chunks[id].srcs.append(int(args['src']))\n",
    "    \n",
    "    def get_chunks(self):\n",
    "        return list(self.chunks.values())\n",
    "\n",
    "def cabocha_reader():\n",
    "    current_chunk_id = None\n",
    "    chunk_store = ChunkStore()\n",
    "    with open('neko.txt.cabocha') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line == 'EOS':\n",
    "                yield chunk_store.get_chunks()\n",
    "                chunk_store = ChunkStore()\n",
    "                continue\n",
    "            if line.startswith('*'): # Chunk\n",
    "                column = line.split(' ')\n",
    "                current_chunk_id = column[1]\n",
    "                dst = int(column[2][:-1]) # remove 'D'\n",
    "                chunk_store.update_chunk(current_chunk_id, dst=dst)\n",
    "                if dst > -1:\n",
    "                    chunk_store.update_chunk(dst, src=current_chunk_id)\n",
    "            else: # Morph\n",
    "                surface, others = line.split('\\t')\n",
    "                others = others.split(',')\n",
    "                chunk_store.update_chunk(current_chunk_id, morph=Morph(surface,  others[6], others[0], others[1]))\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if i == 8:\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            print(j, chunk)\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は\t猫である\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n"
     ]
    }
   ],
   "source": [
    "# 最初の10個を表示\n",
    "total = 0\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if total >= 10:\n",
    "        break\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        if total >= 10:\n",
    "            break\n",
    "        if chunk.dst > -1:\n",
    "            surface = chunk.surface()\n",
    "            dist_surface = chunks[chunk.dst].surface()\n",
    "            if surface == '' or dist_surface == '':\n",
    "                continue\n",
    "            print('{}\\t{}'.format(surface, dist_surface))\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "我々を\t捕えて\n"
     ]
    }
   ],
   "source": [
    "# 最初の10個を表示\n",
    "total = 0\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if total >= 10:\n",
    "        break\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        if total >= 10:\n",
    "            break\n",
    "        if chunk.dst > -1:\n",
    "            if chunk.is_include_noun() and chunks[chunk.dst].is_include_verb():\n",
    "                surface = chunk.surface()\n",
    "                dist_surface = chunks[chunk.dst].surface()\n",
    "                if surface == '' or dist_surface == '':\n",
    "                    continue\n",
    "                print('{}\\t{}'.format(surface, dist_surface))\n",
    "                total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "* 述語に係る助詞を格とする\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "* コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "* 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('case_pattern.txt', 'w') as f:\n",
    "    for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            if chunk.is_include_verb() and len(chunk.srcs) > 0:\n",
    "                joshi = []\n",
    "                for src in chunk.srcs:\n",
    "                    predicate = chunk.predicate()\n",
    "                    if chunks[src].is_include_joshi():\n",
    "                        joshi.append(chunks[src].joshi())\n",
    "                if len(joshi) > 0:\n",
    "                    # print('{}\\t{}'.format(predicate, ' '.join(joshi)))\n",
    "                    line = '{}\\t{}\\n'.format(predicate, ' '.join(joshi))\n",
    "                    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 704 云う\tと\r\n",
      " 452 する\tを\r\n",
      " 333 思う\tと\r\n",
      " 202 ある\tが\r\n",
      " 199 なる\tに\r\n",
      " 188 する\tに\r\n",
      " 175 見る\tて\r\n",
      " 159 する\tと\r\n",
      " 116 する\tが\r\n",
      "  98 見る\tを\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!sort case_pattern.txt | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 452 する\tを\r\n",
      " 188 する\tに\r\n",
      " 159 する\tと\r\n",
      " 116 する\tが\r\n",
      "  88 する\tて を\r\n",
      "  85 する\tは\r\n",
      "  61 する\tを に\r\n",
      "  61 する\tて\r\n",
      "  60 する\tも\r\n",
      "  54 する\tが を\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^する\\s\" case_pattern.txt | sort | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 175 見る\tて\r\n",
      "  98 見る\tを\r\n",
      "  23 見る\tて て\r\n",
      "  20 見る\tから\r\n",
      "  17 見る\tと\r\n",
      "  13 見る\tは て\r\n",
      "  12 見る\tて を\r\n",
      "  12 見る\tで\r\n",
      "  11 見る\tから て\r\n",
      "   9 見る\tに\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^見る\\s\" case_pattern.txt | sort | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 与える\tに を\r\n",
      "   2 与える\tて に を\r\n",
      "   1 与える\tけれども は を\r\n",
      "   1 与える\tとして か\r\n",
      "   1 与える\tが は は と て に を\r\n",
      "   1 与える\tは て に を に\r\n",
      "   1 与える\tは て に を\r\n",
      "   1 与える\tと は て を\r\n",
      "   1 与える\tて は に を\r\n",
      "   1 与える\tは は も\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"^与える\\s\" case_pattern.txt | sort | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "* 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\tどこで\n",
      "つく\tか が\t生れたか 見当が\n",
      "泣く\tで\t所で\n",
      "する\tて は\t泣いて いた事だけは\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n"
     ]
    }
   ],
   "source": [
    "# 最初の10件のみ表示\n",
    "total = 0\n",
    "to = 10\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if total >= to:\n",
    "        break\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        if total >= to:\n",
    "            break\n",
    "        if chunk.is_include_verb() and len(chunk.srcs) > 0:\n",
    "            joshi = []\n",
    "            src_surfaces = []\n",
    "            for src in chunk.srcs:\n",
    "                predicate = chunk.predicate()\n",
    "                if chunks[src].is_include_joshi():\n",
    "                    joshi.append(chunks[src].joshi())\n",
    "                    src_surfaces.append(chunks[src].surface())\n",
    "            if len(joshi) > 0:\n",
    "                line = '{}\\t{}\\t{}'.format(predicate, ' '.join(joshi), ' '.join(src_surfaces))\n",
    "                print(line)\n",
    "                total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "* 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "* 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "* 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "* コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "* コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('sahen_wo_verb.txt', 'w') as f:\n",
    "    for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            if chunk.is_include_sahen_wo() and chunks[chunk.dst].is_include_verb():\n",
    "                predicate = chunk.sahen_wo() + chunks[chunk.dst].predicate()\n",
    "\n",
    "                joshi = []\n",
    "                src_surfaces = []\n",
    "                for src in chunk.srcs:\n",
    "                    if chunks[src].is_include_joshi():\n",
    "                        joshi.append(chunks[src].joshi())\n",
    "                        src_surfaces.append(chunks[src].surface())\n",
    "                for src in chunks[chunk.dst].srcs:\n",
    "                    if src == j:\n",
    "                        continue\n",
    "                    if chunks[src].is_include_joshi():\n",
    "                        joshi.append(chunks[src].joshi())\n",
    "                        src_surfaces.append(chunks[src].surface())\n",
    "\n",
    "                if len(joshi) > 0:\n",
    "                    line = '{}\\t{}\\t{}\\n'.format(predicate, ' '.join(joshi), ' '.join(src_surfaces))\n",
    "                    f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  26 返事をする\r\n",
      "  20 挨拶をする\r\n",
      "  14 話をする\r\n",
      "   8 質問をする\r\n",
      "   8 真似をする\r\n",
      "   8 喧嘩をする\r\n",
      "   5 質問をかける\r\n",
      "   5 相談をする\r\n",
      "   5 注意をする\r\n",
      "   5 昼寝をする\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 sahen_wo_verb.txt | sort | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5 返事をする\tと\r\n",
      "   4 挨拶をする\tと\r\n",
      "   3 質問をかける\tと は\r\n",
      "   3 挨拶をする\tから\r\n",
      "   3 返事をする\tは と\r\n",
      "   3 喧嘩をする\tと\r\n",
      "   2 同情を表する\tと は て\r\n",
      "   2 挨拶をする\tと も\r\n",
      "   2 議論をする\tて\r\n",
      "   2 真似をする\tの\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1,2 sahen_wo_verb.txt | sort | uniq -c | sort -r | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 各文節は（表層形の）形態素列で表現する\n",
    "* パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk_path(chunks, i):\n",
    "    path = []\n",
    "    chunk = chunks[i]\n",
    "    path.append(chunk)\n",
    "    \n",
    "    if chunk.dst != -1:\n",
    "        path += chunk_path(chunks, chunk.dst)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\n",
      "吾輩は ->猫である\n",
      "猫である\n",
      "名前は ->無い\n",
      "どこで ->生れたか ->つかぬ\n",
      "見当が ->つかぬ\n",
      "何でも ->薄暗い ->所で ->泣いて ->記憶している\n",
      "所で ->泣いて ->記憶している\n",
      "ニャーニャー ->泣いて ->記憶している\n",
      "いた事だけは ->記憶している\n"
     ]
    }
   ],
   "source": [
    "# 最初の10件のみ表示\n",
    "total = 0\n",
    "to = 10\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if total >= to:\n",
    "        break\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        if total >= to:\n",
    "            break\n",
    "        if chunk.is_include_noun():\n",
    "            path = chunk_path(chunks, j)\n",
    "            print(' ->'.join([c.surface() for c in path]))\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiiとjj（i<ji<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "* 文節iiとjjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる\n",
    "\n",
    "* 文節iiから構文木の根に至る経路上に文節jjが存在する場合: 文節iiから文節jjのパスを表示\n",
    "* 上記以外で，文節iiと文節jjから構文木の根に至る経路上で共通の文節kkで交わる場合: 文節iiから文節kkに至る直前のパスと文節jjから文節kkに至る直前までのパス，文節kkの内容を\"|\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは -> Y\n",
      "　Xで -> 生れたか | Yが | つかぬ\n",
      "Xでも -> 薄暗い -> Y\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて | Yだけは | 記憶している\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて -> Y\n",
      "Xで -> 泣いて | Yだけは | 記憶している\n",
      "Xで -> 泣いて -> Y\n",
      "X -> 泣いて | Yだけは | 記憶している\n",
      "X -> 泣いて -> Y\n",
      "Xだけは -> Y\n"
     ]
    }
   ],
   "source": [
    "def merge_path(chunks, chunk_path_X, chunk_path_Y):\n",
    "    noun_X = chunk_path_X[0]\n",
    "    noun_Y = chunk_path_Y[0]\n",
    "    ret = []\n",
    "    \n",
    "    # Xで -> 始めて -> Y\n",
    "    if noun_Y in chunk_path_X:\n",
    "        idx = chunk_path_X.index(noun_Y)\n",
    "        for i, c in enumerate(chunk_path_X[:idx]):\n",
    "            if i == 0:\n",
    "                ret.append(c.masked_noun_phrase('X'))\n",
    "            else:\n",
    "                ret.append(' -> ')\n",
    "                ret.append(c.surface())\n",
    "        ret.append(' -> ')\n",
    "        ret.append(noun_Y.masked_noun('Y'))\n",
    "            \n",
    "        return ''.join(ret)\n",
    "    else: #Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "        common_chunks = set(chunk_path_X) & set(chunk_path_Y)\n",
    "        if len(common_chunks) == 1:\n",
    "            common_chunk = list(common_chunks)[0]\n",
    "            idx_X = chunk_path_X.index(common_chunk)\n",
    "            idx_Y = chunk_path_Y.index(common_chunk)\n",
    "            for i, c in enumerate(chunk_path_X[0:idx_X]):\n",
    "                if i == 0:\n",
    "                    ret.append(c.masked_noun_phrase('X'))\n",
    "                else:\n",
    "                    ret.append(' -> ')\n",
    "                    ret.append(c.surface())\n",
    "            ret.append(' | ')\n",
    "            for i, c in enumerate(chunk_path_Y[0:idx_Y]):\n",
    "                if i == 0:\n",
    "                    ret.append(c.masked_noun_phrase('Y'))\n",
    "                else:\n",
    "                    ret.append(' -> ')\n",
    "                    ret.append(c.surface())\n",
    "            ret.append(' | ')\n",
    "            ret.append(common_chunk.surface())\n",
    "            return ''.join(ret)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "# 最初の10件のみ表示\n",
    "total = 0\n",
    "to = 10\n",
    "for i, chunks in enumerate(cabocha_reader(), 1):\n",
    "    if total >= to:\n",
    "        break\n",
    "    noun_chunks = [c for c in chunks if c.is_include_noun()]\n",
    "    for j, chunk in enumerate(noun_chunks):\n",
    "        X_dst = chunk.dst\n",
    "        X_path = chunk_path(chunks, chunks.index(chunk))\n",
    "        for k in range(j+1, len(noun_chunks)):\n",
    "            Y_path = chunk_path(chunks, chunks.index(noun_chunks[k]))\n",
    "            #print(\"X:\", [c.surface() for c in X_path])\n",
    "            #print(\"Y:\", [c.surface() for c in Y_path])\n",
    "            path = merge_path(chunks, X_path, Y_path)\n",
    "            if len(path) > 0:\n",
    "                print(path)\n",
    "                # print(\"\\n\")\n",
    "                total += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
